\chapter{Обзор литературы}
\label{chap:lr}
\chaptermark{Обзор литературы}

В данном разделе рассматриваются четыре основных семейства тензорных разложений: Tucker, Tensor-Train (TT), CANDECOMP/PARAFAC и Robust Tensor PCA (RTPCA). Первые три являются классическими методами, лежащими в основе большинства современных приложений, в то время как RTPCA представляет собой недавно разработанные робастные расширения. Основное внимание уделяется математическим основам, ключевым улучшениям и типичным областям применения.

\section{Методы тензорного разложения}
\label{sec:lr_decomposition_methods}

\subsection*{CANDECOMP / PARAFAC}
\label{subsec:lr_cp}

Данный метод известен под разными названиями: Canonical Decomposition (CANDECOMP), Canonical Polyadic Decomposition (CPD), CP и Parallel Factor Analysis (PARAFAC), все они обозначают один и тот же подход. Метод был предложен Ф. Л. Хитчкоком в 1927 году \cite{Hitchcock1927} и позднее обобщён Кэрроллом и Чангом в 1970 году в контексте многомерного масштабирования \cite{Carroll1970}. 

CANDECOMP/PARAFAC аппроксимирует тензор \(\mathcal{X} \in \mathbb{C}^{I_1 \times \cdots \times I_N}\) суммой ранга-один тензоров:
\[
\mathcal{X} \approx \sum_{r=1}^R a_r^{(1)} \otimes a_r^{(2)} \otimes \cdots \otimes a_r^{(N)},
\]
где \(R\) — ранг разложения, \(a_r^{(n)} \in \mathbb{C}^{I_n}\) — факторные векторы, а \(\otimes\) — тензорное произведение \cite{tensorly_parafac_tucker, tensorly_parafac_2, tensorly_parafac_3}. Метод характеризуется возможной уникальностью решений при выполнении определённых условий, что обеспечивает интерпретируемость факторов, однако подбор ранга и вычислительная эффективность остаются сложными задачами.

\subsection*{Tucker}
\label{subsec:lr_tucker}

Tucker-разложение (Higher-Order SVD, HOSVD) — обобщение матричных методов факторизации на многомерные массивы, предложенное Л. Р. Такером в 1966 году \cite{tucker_method}. Для тензора третьего порядка \(\mathcal{Y} \in \mathbb{C}^{I_1 \times I_2 \times I_3}\) разложение записывается как:
\[
\mathcal{Y} \approx \mathcal{G} \times_1 A \times_2 B \times_3 C,
\]
где \(\mathcal{G} \in \mathbb{C}^{R_1 \times R_2 \times R_3}\) — ядро тензора, а \(A, B, C\) — матрицы факторов, соответствующие режимам \cite{tucker_method}. Такой подход позволяет адаптировать ранги по каждому режиму и выявлять сложные взаимодействия между компонентами, что полезно для анализа, сжатия и шумоподавления. Недостатком является неуниверсальность решения, затрудняющая интерпретацию факторов.

\subsubsection*{Робастное Tucker-разложение с функцией потерь Баррона \cite{barron_loss_tensor_decomposition}}
\label{subsec:lr_barron_loss_tucker}

В работе \cite{barron_loss_tensor_decomposition} предложено усовершенствование Tucker-разложения с использованием робастной функции потерь Баррона \cite{barron2019generaladaptiverobustloss} (с параметром \(\alpha=0\)) для повышения устойчивости к шумам и выбросам. Это обеспечивает более точную аппроксимацию тензоров на реальных данных, содержащих аномалии.

\subsection*{Tensor-Train (TT)}
\label{subsec:lr_tt}

TT-разложение \cite{tensorly_tensor_train} представляет тензор порядка \(d\) в виде цепочки связанных тензоров меньшего порядка (TT-ядра):
\[
\mathcal{X}_{i_1,\ldots,i_d} \approx \sum_{r_1=1}^{R_1} \cdots \sum_{r_{d-1}=1}^{R_{d-1}} G^{(1)}_{i_1,r_1} G^{(2)}_{r_1,i_2,r_2} \cdots G^{(d)}_{r_{d-1},i_d},
\]
где \(G^{(k)}\) — TT-ядра, а \(R_k\) — TT-ранги, определяющие компромисс между точностью и размером представления. TT позволяет существенно уменьшить объём параметров, сохраняя многомерную структуру данных, и широко применяется для сжатия нейросетей и научных вычислений.

\subsection*{Robust Tensor Principal Component Analysis (RTPCA)}
\label{subsec:lr_rtpca}

RTPCA сочетает тензорные разложения с робастным PCA, выделяя низкоранговую структуру данных и одновременно устраняя шумы и выбросы \cite{rtpca_method}. Формулировка задачи:
\[
\min_{\mathcal{L}, \mathcal{S}} \sum_n \|\mathcal{L}_{(n)}\|_* + \lambda \|\mathcal{S}\|_1, \quad \text{при условии } \mathcal{M} = \mathcal{L} + \mathcal{S},
\]
где \(\mathcal{M}\) — наблюдаемый тензор, \(\mathcal{L}\) — низкоранговый компонент, \(\mathcal{S}\) — разреженный шум. Метод эффективен для обработки реальных шумных данных, включая биомедицинские сигналы и видео \cite{rtpca_method}.

\section{Цели и приложения тензорных разложений}
\label{sec:lr_tensor_decomposition_objectives_and_applications}

\subsection*{Цели}
\label{subsec:lr_tensor_decomposition_objectives}

Основными задачами тензорных разложений являются:
\begin{itemize}
    \item Сжатие данных — уменьшение объёма без значительных потерь информации, что критично для изображений и видео \cite{tensor_computation_for_data_analysis};
    \item Эффективное представление — понижение размерности для ускорения вычислений, например, сжатие слоёв нейросетей \cite{stable_low_rank_tensor_decomposition};
    \item Шумоподавление — выделение релевантной информации за счёт удаления шума \cite{tensor_computation_for_data_analysis}.
\end{itemize}

\subsection*{Примеры применения}

\textbf{Стабильное низкоранговое разложение для сжатия сверточных сетей} \cite{stable_low_rank_tensor_decomposition} демонстрирует эффективность сочетания CPD и Tucker для компрессии фильтров CNN (VGG-16, ResNet-18, ResNet-50) с улучшением производительности и снижением потерь точности.

\textbf{Гибридный подход к сжатию нейросетей} \cite{hybrid_tensor_decomposition_c_nn} сочетает TT для сверточных и Hierarchical Tucker для полносвязных слоёв, достигая улучшенных результатов по сравнению с отдельными методами.

\textbf{Методы Cross Tensor Approximation} \cite{cross_tensor_approximation} предлагают альтернативу традиционным разложениям с повышенной вычислительной эффективностью и масштабируемостью при обработке высокоразмерных данных.

---

Данный обзор подчёркивает важность глубокого понимания теоретических основ и актуальных направлений в тензорном анализе для эффективного использования методов в современных научных и инженерных задачах.
