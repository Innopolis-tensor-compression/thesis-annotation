\chapter{Реализация}
\label{chap:implementation}
\chaptermark{Реализация}

Вся кодовая база доступна по ссылке: \url{https://github.com/Innopolis-tensor-compression/tensor-compression-methods}. Проект организован с учетом возможности воспроизводимости: фиксация версий библиотек в \texttt{pyproject.toml}, установленные сиды (\texttt{np/torch/tf.random.seed(42)}), отключённые нефрагментированные эвристики cuDNN и \texttt{TF\_DETERMINISTIC\_OPS=1}.

\begingroup
\sloppy
\subsection*{Использованные среды для экспериментов:}
\begin{itemize}\setlength\itemsep{0.15em}
    \item \emph{Среда A} — WSL 2 (Ubuntu 22.04) на Intel i7-6700K, 32 GB DDR3, NVIDIA GTX 1080 Ti 11 GB; Python 3.11, CUDA-PyTorch 2.0.1, TensorFlow 2.17.  
    \item \emph{Среда B} — Ubuntu 25.04 на Intel i7-13700HK, 32 GB DDR5, NVIDIA RTX 4070 8 GB; Python 3.12, PyTorch 2.5.1 + CUDA.  
\end{itemize}
\endgroup


\subsection*{Алгоритм подбора ранга для Tucker и Tensor-Train}  
Эксперименты реализованы на \texttt{TensorLy}+PyTorch (Tucker, TT) и \texttt{SciPy} оптимизаторах (Nelder–Mead, Powell, SLSQP, differential evolution) с единой функцией потерь $\mathcal{L}(\mathbf r)=
\alpha\,\varepsilon_F(\mathbf r)+
\beta\,(\rho_{\text{target}}-\rho_{\text{actual}}(\mathbf r))^{2},$ ($\alpha{=}1$, $\beta{=}10$).  Для быстрых тестов доступен детерминированный алгоритм покоординатного поиска.

\subsection*{Сравнение реализаций методов декомпозиции}

\textbf{Бенчмарк реализаций методов тензорной декомпозиции.} 
\begingroup
\sloppy
Загрузка тензора (изображения, видео, ЭЭГ); выбор ранга с помощью разработанного алгоритма под целевое сжатие $50\%$; запуск разложения с реализацией на \texttt{TensorLy} или \texttt{T3F}; логирование: время, пик RAM/V\!RAM, ошибка Фробениуса, фактическое сжатие; экспорт в JSON для последующего анализa.  
\endgroup

\textbf{Сравнение по количественным критериям.} Количественную оценку результатов бенчмарка мы проводили посредством совокупности взаимодополняющих методов. Сырые журналы измерений подвергались предварительной обработке, после чего к ним последовательно применялись\,: (1) корреляционный анализ, визуализированный в виде тепловых карт, что позволило выявить устойчивые связи между гиперпараметрами и метриками производительности; (2) понижение размерности методом главных компонент (PCA), обеспечивавшее проекцию многомерных данных в двумерное и трёхмерное пространство без существенной потери информативности; (3) парные диаграммы рассеяния (\textit{pairplot}).

\subsection*{Сжатие слоев нейронных сетей}  
На базе PyTorch создан метод, который: сканирует модель, выделяет свёрточные и транспонированные свёрточные слои; применяет CP, Tucker или гибрид CP+Tucker с автоматически подобранным рангом, исходя из заданного процента сжатия; подменяет исходный слой компактной каскадой; выполняет дообучение для восстановления точности. На VGG-16 и ResNet-18/50 достигнуто 4–8-кратное сокращение параметров при снижении топ-1-точности не более чем на 1 \%.