\chapter{Основные термины}
\label{chap:preliminaries}
\chaptermark{Основные термины}

\textbf{Тензор.} $N$-го порядка (или $N$-мерный) тензор — элемент прямого произведения $N$ векторных пространств. Для $N{=}1$ это вектор, для $N{=}2$ — матрица, при $N\!\ge\!3$ говорят о тензорах высшего порядка.  

\textbf{Форматы хранения.}  
\begin{itemize}\setlength\itemsep{0.2em}
    \item \emph{Dense} — все элементы хранятся явно (базовый случай).  
    \item \emph{Sparse} — сохраняются только ненулевые элементы; эффективно при разреженных данных.  
    \item \emph{Block-Sparse} — группирует ненулевые элементы в блоки, ускоряя операции на структурированных данных.  
    \item \emph{(Супер)симметричные} тензоры используют симметрию по модам, уменьшая избыточность и объём памяти.  
\end{itemize}

\textbf{Тензорное произведение} расширяет понятие матричного (Кронекерова) произведения, комбинируя два тензора в новый, порядок которого равен сумме порядков исходных.  

\textbf{Тензорная контракция} — суммирование по общим индексам двух (или более) тензоров; обобщает скалярное произведение и лежит в основе вычислений в тензорных сетях.  

\textbf{Тензорные сети.} Факторизуют высокоразмерный тензор в граф низкоразмерных «ядер». Классический пример — \emph{Tensor-Train} (Matrix Product State), обеспечивающий полиномиальный рост числа параметров вместо экспоненциального.  

\textbf{Тензорные разложения.} Представляют исходный тензор как сумму или композицию более простых компонент (Tucker, TT, CP, RTPCA). Это ключ к сжатию данных и параметров нейросетей: выбор ранга и формата напрямую определяет компромисс «точность–степень сжатия».  

Перечисленные операции и форматы образуют методологическую основу дальнейших глав, где анализируются их вычислительные свойства и применимость к различным типам данных.