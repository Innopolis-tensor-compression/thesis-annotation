\chapter{Основные термины}
\label{chap:preliminaries}
\chaptermark{Основные термины}

\textbf{Тензор.} $N$-го порядка (или $N$-мерный) тензор — элемент прямого произведения $N$ векторных пространств. Для $N{=}1$ это вектор, для $N{=}2$ — матрица, при $N\!\ge\!3$ говорят о тензорах высшего/высокоразмерного порядка~\cite{tensorly_parafac_tucker}.

\textbf{Структурные форматы тензоров:} \emph{плотные~\cite{tensor_calculus}} - все элементы хранятся явно (базовый случай); \emph{разреженные~\cite{sparse_matrices, sparse_tensor, sparse_and_block_sparse_tensors}} - сохраняются только ненулевые элементы; эффективно при разреженных данных; \emph{блочно - разреженные~\cite{sparse_and_block_sparse_tensors}} - группирует ненулевые элементы в блоки, ускоряя операции на структурированных данных; \emph{(супер)симметричные~\cite{tensor_calculus}} - тензоры используют симметрию по модам, уменьшая избыточность и объём памяти.

\textbf{Тензорное внешнее произведение} расширяет понятие матричного произведения, комбинируя два тензора в новый, порядок которого равен сумме порядков исходных.  

\textbf{Тензорная контракция} — суммирование по общим индексам двух (или более) тензоров; обобщает скалярное произведение и лежит в основе вычислений в тензорных сетях.  

\textbf{Тензорные сети.} Факторизуют высокоразмерный тензор в граф низкоразмерных «ядер». Классический пример — \emph{Tensor-Train} (Matrix Product State), обеспечивающий полиномиальный рост числа параметров вместо экспоненциального.  

\textbf{Методы тензорной декомпозиции.} Представляют исходный тензор как сумму или композицию более простых компонент (Tucker, Tensor-Train (TT), CANDECOMP (CPD/CP) / PARAFAC, RTPCA). Это ключ к сжатию тензоров и слоев нейросетей: выбор ранга и формата напрямую определяет компромисс «точность–степень сжатия». 

Перечисленные операции и форматы образуют методологическую основу дальнейших глав, где анализируются их вычислительные свойства и применимость к различным типам данных.